%------------------------------------------------------------------------------
% Beginning of journal.tex
%------------------------------------------------------------------------------
%
% AMS-LaTeX version 2 sample file for journals, based on amsart.cls.
%
%        ***     DO NOT USE THIS FILE AS A STARTER.      ***
%        ***  USE THE JOURNAL-SPECIFIC *.TEMPLATE FILE.  ***
%
% Replace amsart by the documentclass for the target journal, e.g., tran-l.
%
\documentclass{amsart}

%     If your article includes graphics, uncomment this command.
\usepackage{graphicx}
\usepackage[makeroom]{cancel}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\numberwithin{equation}{section}

%   argmin and argmax
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

%    Absolute value notation
\newcommand{\abs}[1]{\lvert#1\rvert}

%    Blank box placeholder for figures (to avoid requiring any
%    particular graphics capabilities for printing this document).
\newcommand{\blankbox}[2]{%
  \parbox{\columnwidth}{\centering
%    Set fboxsep to 0 so that the actual size of the box will match the
%    given measurements more closely.
    \setlength{\fboxsep}{0pt}%
    \fbox{\raisebox{0pt}[#2]{\hspace{#1}}}%
  }%
}

\begin{document}

\title{Expectation Maximization for Election Prediction in a HMM-like Probabilistic Graphical Model.}

%    Information for first author
\author{Patrick Varin}
%    Address of record for the research reported here
\address{Olin Way, Olin College, Needham, Massachusetts 02492}
%    Current address
%\curraddr{Department of Mathematics and Statistics,
%Case Western Reserve University, Cleveland, Ohio 43403}
\email{patrick.varin@students.olin.edu}
%    \thanks will become a 1st page footnote.
% \thanks{The first author was supported in part by NSF Grant \#000000.}

%    Information for second author
\author{Arjun Aletty}
\address{Olin Way, Olin College, Needham, Massachusetts 02492}
\email{arjun.aletty@students.olin.edu}
% \thanks{Support information for the second author.}

%    General info
\subjclass[2000]{Primary 54C40, 14E20; Secondary 46E25, 20C20}

\date{May 8, 2014}

\dedicatory{This paper is dedicated to our adviser, Paul Ruvolo.}

\keywords{Probabalistic Graphical Model, Election Prediction, Expectation-Maximization, HMM, Baum-Welch}

\begin{abstract}
In this paper we present a probabilistic graphical model (PGM) similar to the Hidden Markov Model in which state transitions are also affected by previous observations. We also develop the Expectation Maximization algorithm specific to this PGM, an analogue to the Baum-Welch algorithm for the HMM. Finally we present this new PGM in the context of of election prediction and we show the performance of the EM algorithm in a series of experiments.
\end{abstract}

\maketitle

\section{Introduction}
The Markov condition is often a good descriptor of a variety of dynamical models. More likely than not, however, we cannot gather complete information from the system in question, in these cases a Hidden Markov Model is often used to analyze the system. Often times these systems are parametrized by a series of unknowns. These unknown parameters can be inferred, however, using the Expectation Maximization algorithm, which chooses the set of parameters that maximizes the likelihood of the observed dataset a famous application of this is in speech recognition, in which a speaker makes a series of utterances, the observed dataset, with the intent of constructing a word, the unknown parameter. The specific implementation of Expectation Maximization for HMMs is called the Baum-Welch algorithm.

There is a set of problems, however, in which the observations can influence the transition to the next state. In an election, for example, popular opinion can be influenced by the outcome of a poll. If an election cycle is considered a series of events in discrete time, the true public opinion can be considered the latent variable that influences the outcome of polls, the observed variables. There are a variety of parameters involved in this model, for example certain polls may have more influence on public opinion and certain polls may have some inherent bias towards or against different candidates.

In this paper we will discuss the computational methods necessary to analyze the graphical model discussed above. We will also revisit this example of using Expectation Maximization to predict the outcome of an election based on poll data.

\section{The Model}
The hidden Markov Model is given by a series of hidden states $\textbf{Z} = \{ Z_t\}$, each of which emits an observable variable $\textbf{X} = \{ X_t\}$. According to the Markov condition state transitions rely only on the previous state, and observations rely only on the immediate state. As a result the model can be described by three probability distributions, the initial state probability $P(Z_1)$, the transition probability $P(Z_t|Z_{t-1})$ and the emission probability $P(X_t|Z_t)$.

For problems in which there is no prior knowledge of the form of these probability distributions it is common to tabulate each of the probabilities in three matrices: $\pi = \{\pi_i\}$, $A = \{ \alpha_{ij}\}$, $B = \{ \beta_{ij}\}$. The initial state probability is given by $P(Z_1 = i) = \pi_i$, the transition probability is given by $P(Z_t = j | Z_{t-1} = i) = \alpha_{ij}$ and the emission probability is given by $P(X_t = j | Z_t = i) = \beta_{ij}$ The Expectation-Maximization algorithm, then attempts to compute each element in these tables. This specific implementation of Expectation-Maximization is called the Baum-Welch algorithm.

The model that we introduce here introduces a new dependency in the state transitions. In this new model these transitions rely not only on the previous state, but also the previous observation. This changes the form of the transition probability to $P(Z_t | Z_{t-1}, t_{x-1})$.

\section{Expectation-Maximization}
In some applications, in which the form of these probability distributions is unknown, it may make sense to parametrize the probability distributions in a similar manner to the HMM described above. In general, however, the structure of Expectation-Maximization is independent of the choice of parametrization and we will use the variable $\theta$ to describe a general set of parameters, e.g., the parametrization used in the Baum-Welch algorithm is $\theta = (\pi,A,B)$.

The Expectation-Maximization procedure is generally separated into two steps, the E-step and the M-step

\subsection*{The E-Step}
The goal of the E-Step is to formulate a fuction, $Q(\theta, \theta')$, the computes the expected value of the log-likelihood of parameters with respect to the probabilities of the latent variables given the last best guess of the parameters.
\[
Q(\theta,\theta') = \textbf{E}_{\textbf{Z}|\textbf{X},\theta'} \left[\log{P(\textbf{X},\textbf{Z}|\theta)}\right]
\]
Computing the joint probability $P(\textbf{X},\textbf{Z}|\theta)$ directly is often not computationally feasible. Using the causal structure of the model, however, we can simplify the joint probability as follows:
\begin{align*}
P(\textbf{X},\textbf{Z}|\theta) &= P(X_{1:T-1},Z_{1:T-1}|\theta)P(X_T,Z_T|X_{1:T-1},Z_{1:T-1},\theta)\\
&= P(X_1,Z_1|\theta)\prod_{t=2}^T P(X_t,Z_t|X_{1:t-1}, Z_{1:t-1},\theta)\\
&= P(X_1,Z_1|\theta)\prod_{t=2}^T P(X_t,Z_t|X_{t-1}, Z_{t-1},\theta)\\
&= P(Z_1|\theta)P(X_1|Z_1,\theta)\prod_{t=2}^T P(Z_t|X_{t-1}, Z_{t-1},\theta) P(X_t|Z_t,\theta)
\end{align*}
or, in grouping like terms (initial, transition, and emission probabilities) it can be expressed more clearly as
\[
P(Z_1|\theta)\prod_{t=2}^T P(Z_t|X_{t-1}, Z_{t-1},\theta) \prod_{t=1}^T P(X_t|Z_t,\theta)
\]
The log-likelihood can therefore be expressed as
\[\log{P(\textbf{X},\textbf{Z}|\theta)} = \log P(Z_1|\theta) + \sum_{t=2}^T \log P(Z_t|X_{t-1}, Z_{t-1},\theta) + \sum_{t=1}^T \log P(X_t|Z_t,\theta)\]
which yields the expression for the expected value of the log-likelihood
\begin{align*}
\textbf{E}_{\textbf{Z}|\textbf{X},\theta'} \left[\log{P(\textbf{X},\textbf{Z}|\theta)}\right] = &\sum_\textbf{Z} P(\textbf{Z}|\textbf{X},\theta')P(Z_1|\theta) + \\
&\sum_\textbf{Z} P(\textbf{Z}|\textbf{X},\theta')\sum_{t=2}^T \log P(Z_t|X_{t-1}, Z_{t-1},\theta) + \\
&\sum_\textbf{Z} P(\textbf{Z}|\textbf{X},\theta')\sum_{t=1}^T \log P(X_t|Z_t,\theta)
\end{align*}
The sum over the set of all possible state sequences $\textbf{Z}$ is exponentially large. However, each of these terms can be simplified to reduce the computational complexity. For example, the first term can be simplified as follows:
\begin{align*}
\sum_\textbf{Z} P(\textbf{Z}|\textbf{X},\theta')P(Z_1|\theta) &= \sum_{Z_{1:T}} P(Z_1|\textbf{X},\theta')P(Z_{2:T}|Z_1,\textbf{X},\theta')P(Z_1|\theta)\\
&= \sum_{Z_1} P(Z_1|\textbf{X},\theta')P(Z_1|\theta)\sum_{Z_{2:T}} P(Z_{2:T}|Z_1,\textbf{X},\theta')\\
&= \sum_{Z_1} P(Z_1|\textbf{X},\theta')P(Z_1|\theta)
\end{align*}
Similarly, the second term simplifies as:
\begin{align*}
\sum_\textbf{Z}& P(\textbf{Z}|\textbf{X},\theta')\sum_{t=2}^T \log P(Z_t|X_{t-1}, Z_{t-1},\theta) = \sum_{Z_{t-1:t}}\sum_{t=2}^T P(Z_{t-1:t}|\textbf{X},\theta') \log P(Z_t|X_{t-1}, Z_{t-1},\theta)
\end{align*}
and the third term simplifies as:
\begin{align*}
\sum_\textbf{Z}& P(\textbf{Z}|\textbf{X},\theta')\sum_{t=1}^T \log P(X_t|Z_t,\theta) = \sum_{Z_t}\sum_{t=1}^T P(Z_t|\textbf{X},\theta') \log P(X_t|Z_t,\theta)
\end{align*}

These simplifications leave the problem computationally tractable, however it introduces two terms that we do not yet know how to compute: $P(Z_t|\textbf{X},\theta')$ and $P(Z_{t-1:t}|\textbf{X},\theta')$. The general approach here is to use the forwards-backwards algorithm.

To begin, we use Bayes rule and note the conditional independence of $X_{t+1:T}$ and $X_{1:t-1}$ given $Z_t$ and $X_t$
\[
P(Z_t|\textbf{X},\theta') = \frac{P(Z_t|X_{1:t},\theta')P(X_{t+1:T}|Z_t,X_t)}{P(X_{t+1:T}|\theta')}
\]
Letting $\boldsymbol\alpha_t$ and $\boldsymbol\beta_t$ represent the forwards and backwards components, respectively we get $\alpha_t(i) = P(Z_t=z_i|X_{1:t},\theta')$ and $\beta_t(i) = P(X_{t+1:T}|Z_t=z_i,X_t)$. In order to compute $\alpha_t$ we expand as follows
\begin{align*}
\alpha_t(i) &= P(Z_t=z_i|X_{1:t}, \theta')\\
&\propto P(Z_t = z_i, X_t | X_{1:t-1}, \theta')\\
&= \sum_j P(Z_t = z_i, Z_{t-1} = z_j, X_t | X_{1:t-1},\theta')\\
&= \sum_j P(Z_t = z_i, X_t | Z_{t-1} = z_j, X_{t-1}, \theta')P(Z_{t-1} = z_j| X_{1:t-1}, \theta')\\
&= P(X_t | Z_t = z_i, \theta') \sum_j P(Z_t = z_i | Z_{t-1} = z_j, X_{t-1}, \theta')\alpha(j)_{t-1}
\end{align*}
Using the new variables $\psi_i = P(X_t | Z_t = z_i, \theta')$ and $\Psi_{ij}(t) = P(Z_t = z_j | Z_{t-1} = z_i, X_{t-1}, \theta')$ we can define $\boldsymbol\alpha_t$ consicely as
\[
\boldsymbol\alpha_t = \boldsymbol\psi \odot \boldsymbol\Psi^T \boldsymbol\alpha_{t-1}
\]
where $\odot$ is an element-wise vector product.

Similarly to the forwards step, the backwards step is computed recursively. Assuming we know $\beta_t$ we can compute $\beta_{t-1}$ as follows.
\begin{align*}
\beta_{t-1}(i) &= P(X_{t:T}|Z_{t-1} = z_i, X_{t-1})\\
&= \sum_j P(X_{t:T}, Z_t = z_j|Z_{t-1} = z_i, X_{t-1})\\
&= \sum_j P(Z_t = z_j, X_t|Z_{t-1} = z_i, X_{t-1})P(X_{t+1:T}| Z_t = z_j, X_{t}, \cancel{Z_{t-1} = z_i}, \cancel{X_{t-1}})\\
&= \sum_j P(Z_t = z_j, X_t|Z_{t-1} = z_i, X_{t-1})\beta_t(j)\\
&= \sum_j P(Z_t = z_j|Z_{t-1} = z_i, X_{t-1}) P(X_t | Z_t = z_j, \cancel{Z_{t-1} = z_i}, \cancel{X_{t-1}})\beta_t(j)\\
&= \sum_j \Psi_{i,j}\psi_j\beta_t(j)
\end{align*}
Which can be written concisely as 
\[
\boldsymbol\Psi(\boldsymbol\psi \odot \boldsymbol\beta_t)
\]

\subsection*{The M-Step}
The purpose of the M-Step is to optimize the log-likelihood function formulated in the E-Step.
\[
\theta_{new}' = \argmax_\theta Q(\theta,\theta')
\]
For some problems this optimization can be computed analytically. Often times though gradient descent, or some other numerical optimization method is used.

\end{document}

% The following is an example of a proof.

% \begin{proof} Set $j(\nu)=\max(I\backslash a(\nu))-1$. Then we have
% \[
% \sum_{i\notin a(\nu)}t_i\sim t_{j(\nu)+1}
%   =\prod^{j(\nu)}_{j=0}(t_{j+1}/t_j).
% \]
% Hence we have
% \begin{equation}
% \begin{split}
% \prod_\nu\biggl(\sum_{i\notin
%   a(\nu)}t_i\biggr)^{\abs{a(\nu-1)}-\abs{a(\nu)}}
% &\sim\prod_\nu\prod^{j(\nu)}_{j=0}
%   (t_{j+1}/t_j)^{\abs{a(\nu-1)}-\abs{a(\nu)}}\\
% &=\prod_{j\ge 0}(t_{j+1}/t_j)^{
%   \sum_{j(\nu)\ge j}(\abs{a(\nu-1)}-\abs{a(\nu)})}.
% \end{split}
% \end{equation}
% By definition, we have $a(\nu(j))\supset c(j)$. Hence, $\abs{c(j)}=n-j$
% implies (5.4). If $c(j)\notin a$, $a(\nu(j))c(j)$ and hence
% we have (5.5).
% \end{proof}

% \begin{quotation}
% This is an example of an `extract'. The magnetization $M_0$ of the Ising
% model is related to the local state probability $P(a):M_0=P(1)-P(-1)$.
% The equivalences are shown in Table~\ref{eqtable}.
% \end{quotation}

% \begin{table}[ht]
% \caption{}\label{eqtable}
% \renewcommand\arraystretch{1.5}
% \noindent\[
% \begin{array}{|c|c|c|}
% \hline
% &{-\infty}&{+\infty}\\
% \hline
% {f_+(x,k)}&e^{\sqrt{-1}kx}+s_{12}(k)e^{-\sqrt{-1}kx}&s_{11}(k)e^
% {\sqrt{-1}kx}\\
% \hline
% {f_-(x,k)}&s_{22}(k)e^{-\sqrt{-1}kx}&e^{-\sqrt{-1}kx}+s_{21}(k)e^{\sqrt
% {-1}kx}\\
% \hline
% \end{array}
% \]
% \end{table}

% \begin{definition}
% This is an example of a `definition' element.
% For $f\in A(X)$, we define
% \begin{equation}
% \mathcal{Z} (f)=\{E\in Z[X]: \text{$f$ is $E^c$-regular}\}.
% \end{equation}
% \end{definition}

% \begin{remark}
% This is an example of a `remark' element.
% For $f\in A(X)$, we define
% \begin{equation}
% \mathcal{Z} (f)=\{E\in Z[X]: \text{$f$ is $E^c$-regular}\}.
% \end{equation}
% \end{remark}

% \begin{example}
% This is an example of an `example' element.
% For $f\in A(X)$, we define
% \begin{equation}
% \mathcal{Z} (f)=\{E\in Z[X]: \text{$f$ is $E^c$-regular}\}.
% \end{equation}
% \end{example}

% \begin{xca}
% This is an example of the \texttt{xca} environment. This environment is
% used for exercises which occur within a section.
% \end{xca}

% The following is an example of a numbered list.

% \begin{enumerate}
% \item First item.
% In the case where in $G$ there is a sequence of subgroups
% \[
% G = G_0, G_1, G_2, \dots, G_k = e
% \]
% such that each is an invariant subgroup of $G_i$.

% \item Second item.
% Its action on an arbitrary element $X = \lambda^\alpha X_\alpha$ has the
% form
% \begin{equation}\label{eq:action}
% [e^\alpha X_\alpha, X] = e^\alpha \lambda^\beta
% [X_\alpha X_\beta] = e^\alpha c^\gamma_{\alpha \beta}
%  \lambda^\beta X_\gamma,
% \end{equation}

% \begin{enumerate}
% \item First subitem.
% \[
% - 2\psi_2(e) =  c_{\alpha \gamma}^\delta c_{\beta \delta}^\gamma
% e^\alpha e^\beta.
% \]

% \item Second subitem.
% \begin{enumerate}
% \item First subsubitem.
% In the case where in $G$ there is a sequence of subgroups
% \[
% G = G_0, G_1, G_2, \ldots, G_k = e
% \]
% such that each subgroup $G_{i+1}$ is an invariant subgroup of $G_i$ and
% each quotient group $G_{i+1}/G_{i}$ is abelian, the group $G$ is called
% \textit{solvable}.

% \item Second subsubitem.
% \end{enumerate}
% \item Third subitem.
% \end{enumerate}
% \item Third item.
% \end{enumerate}

% Here is an example of a cite. See \cite{A}.

% \begin{theorem}
% This is an example of a theorem.
% \end{theorem}

% \begin{theorem}[Marcus Theorem]
% This is an example of a theorem with a parenthetical note in the
% heading.
% \end{theorem}

% \begin{figure}[tb]
% %\blankbox{.6\columnwidth}{5pc}
% \includegraphics{lion.png}
% \caption{This is an example of a figure caption with text.}
% \label{firstfig}
% \end{figure}

% \begin{figure}[tb]
% %\blankbox{.75\columnwidth}{3pc}
% \includegraphics{lion.png}
% \caption{}\label{otherfig}
% \end{figure}

% \section{Some more list types}
% This is an example of a bulleted list.

% \begin{itemize}
% \item $\mathcal{J}_g$ of dimension $3g-3$;
% \item $\mathcal{E}^2_g=\{$Pryms of double covers of $C=\openbox$ with
% normalization of $C$ hyperelliptic of genus $g-1\}$ of dimension $2g$;
% \item $\mathcal{E}^2_{1,g-1}=\{$Pryms of double covers of
% $C=\openbox^H_{P^1}$ with $H$ hyperelliptic of genus $g-2\}$ of
% dimension $2g-1$;
% \item $\mathcal{P}^2_{t,g-t}$ for $2\le t\le g/2=\{$Pryms of double
% covers of $C=\openbox^{C'}_{C''}$ with $g(C')=t-1$ and $g(C'')=g-t-1\}$
% of dimension $3g-4$.
% \end{itemize}

% This is an example of a `description' list.

% \begin{description}
% \item[Zero case] $\rho(\Phi) = \{0\}$.

% \item[Rational case] $\rho(\Phi) \ne \{0\}$ and $\rho(\Phi)$ is
% contained in a line through $0$ with rational slope.

% \item[Irrational case] $\rho(\Phi) \ne \{0\}$ and $\rho(\Phi)$ is
% contained in a line through $0$ with irrational slope.
% \end{description}

% \bibliographystyle{amsplain}
% \begin{thebibliography}{10}

% \bibitem {A} T. Aoki, \textit{Calcul exponentiel des op\'erateurs
% microdifferentiels d'ordre infini.} I, Ann. Inst. Fourier (Grenoble)
% \textbf{33} (1983), 227--250.

% \bibitem {B} R. Brown, \textit{On a conjecture of Dirichlet},
% Amer. Math. Soc., Providence, RI, 1993.

% \bibitem {D} R. A. DeVore, \textit{Approximation of functions},
% Proc. Sympos. Appl. Math., vol. 36,
% Amer. Math. Soc., Providence, RI, 1986, pp. 34--56.

% \end{thebibliography}

%------------------------------------------------------------------------------
% End of journal.tex
%------------------------------------------------------------------------------
